{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set to directory with python scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jendawk/Dropbox (MIT)/C Diff Recurrence Paper/Analyses/scripts\n"
     ]
    }
   ],
   "source": [
    "scripts_path = '/Users/jendawk/Dropbox (MIT)/C Diff Recurrence Paper/Analyses/scripts/'\n",
    "\n",
    "# Path to save figures\n",
    "path_to_save = '/'.join(scripts_path.split('/')[:-2]) + '/'\n",
    "\n",
    "# Path to save supplementary and figure data folders\n",
    "save_path = '/Users/jendawk/Dropbox (MIT)/C Diff Recurrence Paper/'\n",
    "%cd $scripts_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from matplotlib import cm\n",
    "import scipy\n",
    "import  itertools\n",
    "from datetime import datetime\n",
    "from seaborn import clustermap\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import time\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from dataLoader import *\n",
    "from basic_data_methods_helper import *\n",
    "from skbio.stats.distance import permanova, DistanceMatrix, anosim\n",
    "from skbio.stats.distance import mantel\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from matplotlib.collections import LineCollection\n",
    "from Bio import Phylo\n",
    "import re\n",
    "from statistics import mode\n",
    "import seaborn as sns\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis, CoxPHSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "from skbio.diversity.alpha import chao1\n",
    "\n",
    "# Set font for figures\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Arial']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/generic.py:5494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# set data_path to point to directory with data\n",
    "data_path = save_path + \"Data/\"\n",
    "\n",
    "# Option to change filtering criteria\n",
    "dl = dataLoader(path = data_path, pt_perc = {'metabs': .25, '16s': .1, 'scfa': 0, 'toxin':0}, meas_thresh = \n",
    "                {'metabs': 0, '16s': 10, 'scfa': 0, 'toxin':0}, \n",
    "                var_perc = {'metabs': 50, '16s': 5, 'scfa': 0, 'toxin':0}, pt_tmpts = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metabs: 760\n",
      "16s: 237\n",
      "scfa: 8\n",
      "toxin: 2\n",
      "metabs_16s: 997\n",
      "metabs_16s_scfa: 997\n",
      "metabs_toxin: 762\n"
     ]
    }
   ],
   "source": [
    "# Number of features in each data set\n",
    "for key in dl.week.keys():\n",
    "    print(key + ': ' + str(dl.week[key][0]['x'].shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 1 - Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n"
     ]
    }
   ],
   "source": [
    "targets = pd.Series(dl.targets_by_pt)\n",
    "targets = targets.replace('Cleared','Non-recurrer')\n",
    "targets = targets.replace('Recur', 'Recurrer')\n",
    "demo_dict = {}\n",
    "for feat in dl.demographics.columns.values:\n",
    "    if len(np.unique(dl.demographics[feat]))<=4 or (dl.demographics[feat].dtypes!=int and dl.demographics[feat].dtypes!=float):\n",
    "        for cat in np.unique(dl.demographics[feat]):\n",
    "            N = np.sum((dl.demographics[feat]==cat)*(targets=='Non-recurrer'))\n",
    "            c_re = np.sum((dl.demographics[feat]==cat)*(targets=='Recurrer'))\n",
    "            demo_dict[(feat,cat)] = {'N_nonRec': N, 'N_Rec': c_re}\n",
    "    else:\n",
    "        demo_dict[(feat,'Mean')] = {'N': np.mean(dl.demographics[feat])}\n",
    "        demo_dict[(feat,'STD')] = {'N': np.std(dl.demographics[feat])}\n",
    "        demo_dict[(feat,'Median')] = {'N': np.median(dl.demographics[feat])}\n",
    "        demo_dict[(feat,'Range')] = {'N': (np.min(dl.demographics[feat]),np.max(dl.demographics[feat]))}\n",
    "        \n",
    "df = pd.DataFrame(demo_dict).T\n",
    "if not os.path.isdir(save_path + 'Main Tables'):\n",
    "    os.mkdir(save_path + 'Main Tables')\n",
    "df.to_csv(save_path + 'Main Tables/Table 1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 1 - # Recurrers at each timepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_dict = {}\n",
    "for week in dl.week['metabs'].keys():\n",
    "    rc_dict[week]={}\n",
    "    targets = dl.week['metabs'][week]['y']\n",
    "    rc_dict[week]['Recurrers'] = np.sum(targets=='Recurrer')\n",
    "    rc_dict[week]['Non-Recurrers'] = np.sum(targets=='Non-recurrer')\n",
    "if not os.path.isdir(path_to_save + 'output_tables'):\n",
    "    os.mkdir(path_to_save + 'output_tables')\n",
    "pd.DataFrame(rc_dict).T.to_csv(save_path + 'Main Tables/Table 2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate analysis for demographics and clinical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(path_to_save + 'univariate_analysis'):\n",
    "    os.mkdir(path_to_save + 'univariate_analysis')\n",
    "\n",
    "# Univariate analysis for demographics\n",
    "targets = dl.keys['metabs']['targets_by_pt']\n",
    "x = dl.demographics\n",
    "xx = dl.demographics[['Age','Sex','Race','BMI','Smoking status']]\n",
    "df_out = univariate_w_chi2(xx, targets)\n",
    "df_out.to_csv(path_to_save + 'univariate_analysis/demographics.csv')\n",
    "\n",
    "# Univariate analysis for clinical variables\n",
    "targets = dl.keys['metabs']['targets_by_pt']\n",
    "x = dl.clinical\n",
    "df_out = univariate_w_chi2(x, targets)\n",
    "df_out.to_csv(path_to_save + 'univariate_analysis/clinical.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export 16S data for DESeq2 in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_save_data=save_path + '/GenFigure&AnalysesData/'\n",
    "\n",
    "if not os.path.isdir(path_to_save + 'scripts/inputs/'):\n",
    "    os.mkdir(path_to_save + 'scripts/inputs/')\n",
    "    \n",
    "if not os.path.isdir(path_to_save + 'scripts/inputs/DEseq2'):\n",
    "    os.mkdir(path_to_save + 'scripts/inputs/DEseq2')\n",
    "\n",
    "weeks = [0,1,2]\n",
    "plot = True\n",
    "\n",
    "data_to_save = {}\n",
    "key = '16s'\n",
    "if key not in data_to_save.keys():\n",
    "    data_to_save[key] = {}\n",
    "for week in weeks:\n",
    "#     if not isinstance(week, list):\n",
    "    x = dl.week_filt[key][week]['x']\n",
    "    y = (dl.week_filt[key][week]['y']=='Recurrer').astype('float')\n",
    "#     else:\n",
    "#         x,y,t = get_slope_data(dl.week_filt[key], week)\n",
    "        \n",
    "    if not os.path.isdir('inputs/DEseq2/'):\n",
    "        os.mkdir('inputs/DEseq2/')\n",
    "    x.to_csv(path_to_save +'scripts/inputs/DEseq2/counts_' + str(week).replace('.','-') + '.csv')\n",
    "    y.to_csv(path_to_save +'scripts/inputs/DEseq2/col_' + str(week).replace('.','-') + '.csv')\n",
    "    yy = pd.DataFrame(y, columns = ['Outcome'])\n",
    "    both = pd.concat([x, yy], 1)\n",
    "    if 'Week ' + str(week) not in data_to_save[key].keys():\n",
    "        data_to_save[key]['Week ' + str(week)] = {}\n",
    "    data_to_save[key]['Week ' + str(week)]['RvNR'] = both\n",
    "    \n",
    "# Export for DEseq2 in R\n",
    "weeks = [(0,1),(1,2)]\n",
    "key = '16s'\n",
    "for outcome in ['Recurrer', 'Non-recurrer']:\n",
    "    for week_pair in weeks:\n",
    "        col0 = dl.week_filt[key][week_pair[0]]['x'].columns.values\n",
    "        col1 = dl.week_filt[key][week_pair[1]]['x'].columns.values\n",
    "        col_all = np.unique(np.concatenate([col0, col1]))\n",
    "        ix0, ix1 = dl.week_filt[key][week_pair[0]]['x'].index.values, dl.week_filt[key][week_pair[1]]['x'].index.values\n",
    "        \n",
    "        x0, y0 = dl.keys[key]['data'][col_all].loc[ix0], dl.week_filt[key][week_pair[0]]['y']\n",
    "        x1, y1 = dl.keys[key]['data'][col_all].loc[ix1], dl.week_filt[key][week_pair[1]]['y']\n",
    "        x0 = x0.loc[y0.values == outcome]\n",
    "        x1 = x1.loc[y1.values == outcome]\n",
    "        x = pd.concat([x0, x1])\n",
    "        y = [week_pair[0]]*x0.shape[0] + [week_pair[1]]*x1.shape[0]\n",
    "        y = pd.Series((np.array(y)==week_pair[1]).astype('float'), index = x.index.values)\n",
    "        if not os.path.isdir('inputs/DEseq2/'):\n",
    "            os.mkdir('inputs/DEseq2/')\n",
    "        x.to_csv(path_to_save +'scripts/inputs/DEseq2/counts_' + outcome +str(week_pair[0]) + '_' + str(week_pair[1]) + '.csv')\n",
    "        y.to_csv(path_to_save +'scripts/inputs/DEseq2/col_' + outcome + str(week_pair[0]) + '_' + str(week_pair[1]) + '.csv')\n",
    "        \n",
    "        yy = pd.DataFrame(y, columns = ['Outcome'])\n",
    "        both = pd.concat([x, yy], 1)\n",
    "        if outcome not in data_to_save[key].keys():\n",
    "            data_to_save[key][outcome] = {}\n",
    "        data_to_save[key][outcome]['Week ' + str(week_pair[0]) + 'v' + str(week_pair[1])] = both\n",
    "        \n",
    "for key in data_to_save.keys():\n",
    "    for key2 in data_to_save[key].keys():\n",
    "        for key3 in data_to_save[key][key2].keys():\n",
    "            val = pd.DataFrame(data_to_save[key][key2][key3])\n",
    "            if os.path.isfile(path_to_save_data + '/' + 'univ_analysis_data.xlsx'):\n",
    "                with pd.ExcelWriter(path_to_save_data + '/' + 'univ_analysis_data.xlsx', mode='a', engine='openpyxl') as writer:\n",
    "                    val.to_excel(writer, sheet_name='_'.join([key, key2, key3.replace(' ','_')]))\n",
    "            else:\n",
    "                with pd.ExcelWriter(path_to_save_data + '/' + 'univ_analysis_data.xlsx', mode = 'w', engine='openpyxl') as writer:\n",
    "                    val.to_excel(writer, sheet_name='_'.join([key, key2, key3.replace(' ','_')]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat DEseq2 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7bdd9b5b2281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdf_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_deseq2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdf_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'padj'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtaxa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_taxa_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_sorted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOTU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mdf_sorted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Taxa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox (MIT)/C Diff Recurrence Paper/Analyses/scripts/helper.py\u001b[0m in \u001b[0;36mreturn_taxa_names\u001b[0;34m(sequences, tax_dat)\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m                 \u001b[0mtdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m                 \u001b[0mtd_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtdat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0mtd_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtd_out\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'nan'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0mnew_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_currow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnew_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         ]\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_multiget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             val = sanitize_array(\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             )\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;31m# we will try to copy be-definition here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCExtensionArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36m_try_cast\u001b[0;34m(arr, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    597\u001b[0m         ):\n\u001b[1;32m    598\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_1d_object_array_from_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_1d_ndarray_preserving_na\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1514\u001b[0m     \"\"\"\n\u001b[1;32m   1515\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1516\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtensionDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/dtypes/base.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mdtype_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m                 \u001b[0mdtype_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtensionDtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set path_deseq2 to path with R results\n",
    "path_deseq2 = path_to_save + 'univariate_analysis/16s/deseq2/'\n",
    "\n",
    "weeks = [0,1,2]\n",
    "plot = False\n",
    "\n",
    "key = '16s'\n",
    "    \n",
    "if not os.path.isdir(path_to_save + 'univariate_analysis/' + key):\n",
    "    os.mkdir(path_to_save + 'univariate_analysis/' + key)\n",
    "for file in os.listdir(path_deseq2):\n",
    "    if file == '.DS_Store':\n",
    "        continue\n",
    "    week = file.split('res')[-1].split('.')[0]\n",
    "    df_r = pd.read_csv(path_deseq2 + file, index_col = 0)\n",
    "    df_sorted = df_r.sort_values(by = 'padj')\n",
    "    taxa = return_taxa_names(df_sorted.OTU)\n",
    "    df_sorted.insert(1, \"Taxa\", taxa, allow_duplicates=True)\n",
    "    \n",
    "    df_sorted.insert(2,\"Direction\", np.zeros(df_sorted.shape[0]).astype(str), True)\n",
    "    \n",
    "    if '_' in file:\n",
    "        # Higher in cleared group (0) or first week\n",
    "        df_sorted[\"Direction\"].loc[df_sorted[\"log2fold\"]<0] = 'Higher in week ' + file.split('_')[0].split('rer')[1]\n",
    "\n",
    "        # Higher in recurred group (1) or second week\n",
    "        df_sorted[\"Direction\"].loc[df_sorted[\"log2fold\"]>0] = 'Higher in week ' + file.split('_')[1].split('.')[0]\n",
    "    else:\n",
    "        # Higher in cleared group (0) or first week\n",
    "        df_sorted[\"Direction\"].loc[df_sorted[\"log2fold\"]<0] = 'Decreased Risk'\n",
    "\n",
    "        # Higher in recurred group (1) or second week\n",
    "        df_sorted[\"Direction\"].loc[df_sorted[\"log2fold\"]>0] = 'Increased Risk'\n",
    "    \n",
    "    df_sorted = df_sorted.set_index(\"OTU\")\n",
    "    if plot:\n",
    "        for metab in df_sorted.index.values:\n",
    "            if df_sorted['padj'][metab] < .1:\n",
    "                plot_metab_over_time(metab, df_sorted['padj'][metab], dl.keys[key], path_to_save + 'univariate_analysis/' + \n",
    "                                     key + '/' + str(week) + '_deseq2_')\n",
    "    df_sorted.to_csv(path_to_save+'univariate_analysis/' + key + '/deseq2_' + key + str(week) + '.csv')\n",
    "    plt.figure()\n",
    "    plt.hist(df_sorted['padj'], bins = np.arange(0,1.05,.05))\n",
    "    plt.xlabel('p-values')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(key + ', week ' + str(week))\n",
    "    plt.savefig(path_to_save+'univariate_analysis/' + key + '/deseq2_' + key + str(week) + '.pdf')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate analysis for metabolites & SCFAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Univariate analysis for metabolites and SCFAs\n",
    "path_to_save_data=save_path + '/GenFigure&AnalysesData/'\n",
    "\n",
    "weeks = [0,1,2]\n",
    "week_pairs = [(0,1),(1,2)]\n",
    "plot = False\n",
    "data_to_save = {}\n",
    "for key in ['metabs', 'scfa']:\n",
    "    data_to_save[key] = {}\n",
    "    if not os.path.isdir(path_to_save + 'univariate_analysis/' + key):\n",
    "        os.mkdir(path_to_save + 'univariate_analysis/' + key)\n",
    "    for outcome in ['Recurrer', 'Non-recurrer']:\n",
    "        if outcome not in data_to_save[key].keys():\n",
    "            data_to_save[key][outcome] = {}\n",
    "        for week_pair in week_pairs:\n",
    "            x0, y0 = dl.week[key][week_pair[0]]['x'], dl.week[key][week_pair[0]]['y']\n",
    "            x0 = x0.loc[y0.values == outcome]\n",
    "            x1, y1 = dl.week[key][week_pair[1]]['x'], dl.week[key][week_pair[1]]['y']\n",
    "            x1 = x1.loc[y1.values == outcome]\n",
    "\n",
    "            xboth = list(set(x0.columns.values).intersection(set(x1.columns.values)))\n",
    "            x = pd.concat([x0, x1])\n",
    "            if 'scfa' in key:\n",
    "                x = x.drop('Caproate', axis = 1)\n",
    "            y = [week_pair[0]]*x0.shape[0] + [week_pair[1]]*x1.shape[0]\n",
    "            y = pd.Series((np.array(y)==week_pair[1]).astype('float'), index = x.index.values)\n",
    "            \n",
    "            yy = pd.DataFrame(y, columns = ['Outcome'])\n",
    "            both = pd.concat([x, yy], 1)\n",
    "            data_to_save[key][outcome]['Week ' + str(week_pair[0]) + 'v' + str(week_pair[1])] = both\n",
    "            \n",
    "            df = univariate_w_chi2(x,y)\n",
    "            df2 = univariate_w_chi2(x,y, method = 'ttest')\n",
    "            if not os.path.isdir(path_to_save+'univariate_analysis/' + key ):\n",
    "                os.mkdir(path_to_save+'univariate_analysis/' + key)\n",
    "\n",
    "            df.insert(2,'Direction', np.zeros(df.shape[0]).astype(str))\n",
    "            df2.insert(2,'Direction', np.zeros(df2.shape[0]).astype(str))\n",
    "\n",
    "            # Higher in cleared group (0) or first week\n",
    "            df[\"Direction\"].loc[df[\"direction\"].astype('float64')<0] = 'Higher in week ' + str(week_pair[0])\n",
    "            df2[\"Direction\"].loc[df2[\"direction\"].astype('float64')<0] = 'Higher in week ' + str(week_pair[0])\n",
    "            # Higher in recurred group (1) or second week\n",
    "            df[\"Direction\"].loc[df[\"direction\"].astype('float64')>0] = 'Higher in week ' + str(week_pair[1])\n",
    "            df2[\"Direction\"].loc[df2[\"direction\"].astype('float64')>0] = 'Higher in week ' + str(week_pair[1])\n",
    "\n",
    "            df.to_csv(path_to_save+'univariate_analysis/' + key + '/ranksum_' + key + str(week_pair[0]) + '_' + \n",
    "                       str(week_pair[1]) + '_' + outcome + '.csv')\n",
    "            df2.to_csv(path_to_save+'univariate_analysis/' + key + '/ttest_' + key + str(week_pair[0]) + '_' + \n",
    "                       str(week_pair[1]) + '_' + outcome + '.csv')\n",
    "            plt.figure()\n",
    "            plt.hist(df2['BH corrected'], bins = np.arange(0,1.05,.05))\n",
    "            plt.xlabel('p-values')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title(key + ', week ' + str(week_pair))\n",
    "            plt.savefig(path_to_save+'univariate_analysis/' + key + '/ttest_' + key + str(week_pair[0]) + '_' + \n",
    "                       str(week_pair[1]) + '_' + outcome + '.pdf')\n",
    "            plt.close()\n",
    "#             with pd.ExcelWriter(path_save + 'Table 5 - Predictive Results.xlsx', mode = 'a') as writer:\n",
    "#                 df.to_excel(writer, sheet_name = 'A. Results')\n",
    "        \n",
    "    for week in weeks:\n",
    "        if week not in data_to_save[key].keys():\n",
    "            data_to_save[key]['Week ' + str(week)] = {}\n",
    "\n",
    "        if not isinstance(week, list):\n",
    "            x, y = dl.week[key][week]['x'], dl.week[key][week]['y']\n",
    "            \n",
    "            yy = pd.Series((np.array(y)=='Recurrer').astype('float'), index = x.index.values)\n",
    "            yy = pd.DataFrame(yy, columns = ['Outcome'])\n",
    "            both = pd.concat([x, yy], 1)\n",
    "            data_to_save[key]['Week ' + str(week)]['RvNR'] = both\n",
    "            if 'scfa' in key:\n",
    "                x = x.drop('Caproate', axis = 1)\n",
    "            if 'toxin' in key:\n",
    "                x = x.iloc[:,-4:]\n",
    "            df = univariate_w_chi2(x,y)\n",
    "            df2 = univariate_w_chi2(x,y, method = 'ttest')\n",
    "        else:\n",
    "            x,y,t = get_slope_data(dl.week[key], week)\n",
    "            if 'scfa' in key:\n",
    "                x = x.drop('Caproate', axis = 1)\n",
    "            if 'toxin' in key:\n",
    "                x = x.iloc[:,-4:]\n",
    "            df = univariate_w_chi2(x,y)\n",
    "            df2 = univariate_w_chi2(x,y,method = 'ttest')\n",
    "        y = pd.DataFrame(y, columns = ['Outcome'])\n",
    "        both = pd.concat([x, yy], 1)\n",
    "        data_to_save[key]['Week ' + str(week)]['RvNR'] = both\n",
    "            \n",
    "        if not os.path.isdir(path_to_save+'univariate_analysis/' + key):\n",
    "            os.mkdir(path_to_save+'univariate_analysis/' + key)\n",
    "        if plot:\n",
    "            for metab in df2.index.values:\n",
    "                if df2['BH corrected'][metab] < .05:\n",
    "                    try:\n",
    "                        plot_metab_over_time(metab, df2['BH corrected'][metab], dl.keys[key], path_to_save + 'univariate_analysis/' + \n",
    "                                             key + '/' + str(week) + '_')\n",
    "                        print('plotted ' + metab)\n",
    "                    except:\n",
    "                        plot_metab_over_time(metab, df2['BH corrected'][metab], dl.keys['metabs'], path_to_save + 'univariate_analysis/' + \n",
    "                                             key + '/' + str(week) + '_')\n",
    "                        print('plotted ' + metab)                    \n",
    "        \n",
    "        df.insert(2,'Direction', np.zeros(df.shape[0]).astype(str))\n",
    "        df2.insert(2,'Direction', np.zeros(df2.shape[0]).astype(str))\n",
    "        \n",
    "        # Higher in cleared group (0) or first week\n",
    "        df[\"Direction\"].loc[df[\"direction\"].astype('float64')<0] = 'Decreased Risk'\n",
    "        df2[\"Direction\"].loc[df2[\"direction\"].astype('float64')<0] = 'Decreased Risk'\n",
    "\n",
    "        # Higher in recurred group (1) or second week\n",
    "        df[\"Direction\"].loc[df[\"direction\"].astype('float64')>0] = 'Increased Risk'\n",
    "        df2[\"Direction\"].loc[df2[\"direction\"].astype('float64')>0] = 'Increased Risk'\n",
    "        \n",
    "        df.to_csv(path_to_save+'univariate_analysis/' + key + '/ranksum_' + key + str(week) + '.csv')\n",
    "        df2.to_csv(path_to_save+'univariate_analysis/' + key + '/ttest_' + key + str(week) + '.csv')\n",
    "        plt.figure()\n",
    "        plt.hist(df2['BH corrected'], bins = np.arange(0,1.05,.05))\n",
    "        plt.xlabel('p-values')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(key + ', week ' + str(week))\n",
    "        plt.savefig(path_to_save+'univariate_analysis/' + key + '/ttest_' + key + str(week) + '.pdf')\n",
    "        plt.close()\n",
    "        \n",
    "        \n",
    "for key in data_to_save.keys():\n",
    "    for key2 in data_to_save[key].keys():\n",
    "        for key3 in data_to_save[key][key2].keys():\n",
    "            val = pd.DataFrame(data_to_save[key][key2][key3])\n",
    "            if os.path.isfile(path_to_save_data + '/' + 'univ_analysis_data.xlsx'):\n",
    "                with pd.ExcelWriter(path_to_save_data + '/' + 'univ_analysis_data.xlsx', mode='a', engine='openpyxl') as writer:\n",
    "                    val.to_excel(writer, sheet_name='_'.join([key, key2, key3.replace(' ','_')]))\n",
    "            else:\n",
    "                with pd.ExcelWriter(path_to_save_data + '/' + 'univ_analysis_data.xlsx', mode = 'w', engine='openpyxl') as writer:\n",
    "                    val.to_excel(writer, sheet_name='_'.join([key, key2, key3.replace(' ','_')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save compact univariate analysis to supplemental tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from string import ascii_uppercase\n",
    "path_univariate = path_to_save + 'univariate_analysis/'\n",
    "path_save = save_path + '/Supplemental Tables/'\n",
    "taxa_labels = pd.read_csv(path_to_save + 'scripts/inputs/' + 'taxa_labels.csv', index_col = [0])\n",
    "\n",
    "k = 3\n",
    "for dtype in os.listdir(path_univariate):\n",
    "    if dtype == 'toxin':\n",
    "        continue\n",
    "    labs = []\n",
    "    df_out = {}\n",
    "\n",
    "    fname = 'Table 2. Univariate Analysis.xlsx'\n",
    "\n",
    "    i = 0\n",
    "    if dtype == '.DS_Store':\n",
    "        continue\n",
    "        \n",
    "    if '.csv' in dtype:\n",
    "        continue\n",
    "    for file in os.listdir(path_univariate + dtype):\n",
    "        if 'kruskal' in file or 'ranksum' in file or '[' in file or '.DS_Store' in file:\n",
    "            continue\n",
    "        if '.csv' not in file:\n",
    "            continue\n",
    "#         print(i)\n",
    "#         print(file)\n",
    "        timepoint = file.split('.')[0].split(dtype)[1]\n",
    "        if 'rer' in timepoint:\n",
    "            if dtype == '16s':\n",
    "                label = timepoint.split('rer')[0] + 'rer;Week' + 'vs'.join(timepoint.split('rer')[1].split('_'))\n",
    "            else:\n",
    "                label = timepoint.split('_')[-1] + ';Week' + 'vs'.join(timepoint.split('_')[:-1])\n",
    "            lab_out = ('Intra-group',label)\n",
    "        else:\n",
    "            label = 'Week' + timepoint + ';R_vs_NR'\n",
    "            lab_out = ('Inter-group',label)\n",
    "        \n",
    "        df = pd.read_csv(path_univariate + dtype + '/' + file, index_col = [0])\n",
    "        labs.append(lab_out)\n",
    "        if dtype == '16s':\n",
    "            df = df.sort_values(by = 'padj')\n",
    "            if np.sum(df['padj']<0.1)==0:\n",
    "                df = df.iloc[:10,:]\n",
    "            else:\n",
    "                df = df.loc[df['padj']<0.1]\n",
    "            seq_labels = taxa_labels['labels'].loc[df.index.values]\n",
    "            for ix in df.index.values:\n",
    "                df_out[(lab_out[0], lab_out[1], ix)] = {'Labels': seq_labels.loc[ix], 'FDR': df['padj'].loc[ix], \n",
    "                                                        'Direction': df['Direction'].loc[ix], \n",
    "                          'log2fold': df['log2fold'].loc[ix],\n",
    "                         'Taxonomy-RDP': taxa_labels['taxa_rdp'].loc[ix], \n",
    "                          }\n",
    "#             df_out[lab_out] = pd.DataFrame(df_out[lab_out])\n",
    "#             df_out[lab_out] = df_out[lab_out].set_index('Labels')\n",
    "        elif dtype == 'metabs':\n",
    "            df = df.sort_values(by = 'BH corrected')\n",
    "            if np.sum(df['BH corrected']<0.1)==0:\n",
    "                df = df.iloc[:10,:]\n",
    "            else:\n",
    "                df = df.loc[df['BH corrected']<0.1]\n",
    "            for ix in df.index.values:\n",
    "                df_out[(lab_out[0], lab_out[1], ix)] = {'FDR': df['BH corrected'].loc[ix], \n",
    "                                                        'Direction': df['Direction'].loc[ix], \n",
    "                          'test statistic': df['test statistic'].loc[ix], \n",
    "                         'Super Pathway': dl.col_mat_mets['SUPER_PATHWAY'].loc[ix],\n",
    "                         'Sub Pathway': dl.col_mat_mets['SUB_PATHWAY'].loc[ix]}\n",
    "#             df_out[lab_out] = pd.DataFrame(df_out[lab_out])\n",
    "#             df_out[lab_out] = df_out[lab_out].set_index('names')\n",
    "        else:\n",
    "            df = df.sort_values(by = 'BH corrected')\n",
    "#             df = df.loc[df['BH corrected']<0.1]\n",
    "            for ix in df.index.values:\n",
    "                df_out[(lab_out[0], lab_out[1], ix)] = {'FDR': df['BH corrected'].loc[ix], \n",
    "                                   'Direction': df['Direction'].loc[ix], \n",
    "                               'test statistic': df['test statistic'].loc[ix]}\n",
    "            \n",
    "        \n",
    "    df_out = pd.DataFrame(df_out).T.sort_index()\n",
    "    if os.path.exists(path_save + fname):\n",
    "        with pd.ExcelWriter(path_save + fname, mode = 'a') as writer:\n",
    "            df_out.to_excel(writer, sheet_name = dtype.capitalize())\n",
    "    else:\n",
    "        with pd.ExcelWriter(path_save + fname) as writer:\n",
    "            df_out.to_excel(writer, sheet_name = dtype.capitalize())\n",
    "#         i+= 1\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save full univariate analysis to Data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from string import ascii_uppercase\n",
    "path_univariate = path_to_save + 'univariate_analysis/'\n",
    "path_save = save_path + '/Data/Univariate analyses/'\n",
    "if not os.path.isdir(path_save):\n",
    "    os.mkdir(path_save)\n",
    "taxa_labels = pd.read_csv(path_to_save + 'scripts/inputs/' + 'taxa_labels.csv', index_col = [0])\n",
    "\n",
    "k = 3\n",
    "for dtype in os.listdir(path_univariate):\n",
    "    fname = 'Table ' + str(k + 1) + '-' + dtype.capitalize() + ' Univariate Analysis.xlsx'\n",
    "\n",
    "    \n",
    "    if dtype == '.DS_Store':\n",
    "        continue\n",
    "    if '.csv' in dtype:\n",
    "        os.rename(path_univariate + dtype, path_save + dtype)\n",
    "        continue\n",
    "    i = 0\n",
    "    for file in os.listdir(path_univariate + dtype):\n",
    "        if 'kruskal' in file or 'ranksum' in file or '[' in file or '.DS_Store' in file:\n",
    "            continue\n",
    "        if '.csv' not in file:\n",
    "            continue\n",
    "#         print(i)\n",
    "#         print(file)\n",
    "        \n",
    "        df = pd.read_csv(path_univariate + dtype + '/' + file, index_col = [0])\n",
    "        if dtype == '16s':\n",
    "            seq_labels = taxa_labels['labels'].loc[df.index.values]\n",
    "            df_out = {'FDR': df['padj'], 'Direction': df['Direction'], \n",
    "                      'log2fold': df['log2fold'],'Taxonomy-Silva': taxa_labels['taxa_silva'].loc[df.index.values], \n",
    "                     'Taxonomy-RDP': taxa_labels['taxa_rdp'].loc[df.index.values], \n",
    "                      'Sequences': df.index.values, 'Labels': seq_labels}\n",
    "            df_out = pd.DataFrame(df_out)\n",
    "            df_out = df_out.set_index('Labels')\n",
    "        elif dtype == 'metabs':\n",
    "            df_out = {'FDR': df['BH corrected'], 'Direction': df['Direction'], \n",
    "                      'test statistic': df['test statistic'], 'PubChem#': dl.col_mat_mets['PUBCHEM'].loc[df.index.values],\n",
    "                     'KEGG#': dl.col_mat_mets['PUBCHEM'].loc[df.index.values], 'HMDB#': \n",
    "                      dl.col_mat_mets['HMDB'].loc[df.index.values], \n",
    "                     'Super Pathway': dl.col_mat_mets['SUPER_PATHWAY'].loc[df.index.values],\n",
    "                     'Sub Pathway': dl.col_mat_mets['SUB_PATHWAY'].loc[df.index.values], 'names':df.index.values}\n",
    "            df_out = pd.DataFrame(df_out)\n",
    "            df_out = df_out.set_index('names')\n",
    "        elif dtype == 'toxin':\n",
    "            df_out = {'FDR': df['BH corrected'], 'Direction': df['Direction'], 'test statistic': df['test statistic'],\n",
    "                     'method': df['method']}\n",
    "            df_out = pd.DataFrame(df_out)\n",
    "        else:\n",
    "            df_out = {'FDR': df['BH corrected'], 'Direction': df['Direction'], 'test statistic': df['test statistic']}\n",
    "            df_out = pd.DataFrame(df_out)\n",
    "        \n",
    "        timepoint = file.split('.')[0].split(dtype)[1]\n",
    "\n",
    "        if 'rer' in timepoint:\n",
    "            if dtype == '16s':\n",
    "                label = timepoint.split('rer')[0] + 'rer;Week' + 'vs'.join(timepoint.split('rer')[1].split('_'))\n",
    "            else:\n",
    "                label = timepoint.split('_')[-1] + ';Week' + 'vs'.join(timepoint.split('_')[:-1])\n",
    "        else:\n",
    "            label = 'Week' + timepoint + ';R_vs_NR'\n",
    "        \n",
    "        nm = ascii_uppercase[i]\n",
    "        \n",
    "        if os.path.exists(path_save + fname):\n",
    "            with pd.ExcelWriter(path_save + fname, mode = 'a') as writer:\n",
    "                df_out.to_excel(writer, sheet_name = nm + '.' + label)\n",
    "        else:\n",
    "            with pd.ExcelWriter(path_save + fname) as writer:\n",
    "                df_out.to_excel(writer, sheet_name = nm + '.' + label)\n",
    "        i+= 1\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CodeBase)",
   "language": "python",
   "name": "codebase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

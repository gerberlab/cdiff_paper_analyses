{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set to directory with python scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jendawk/Dropbox (MIT)/C Diff Recurrence Paper/Analyses/scripts\n"
     ]
    }
   ],
   "source": [
    "%cd '/Users/jendawk/Dropbox (MIT)/C Diff Recurrence Paper/Analyses/scripts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from matplotlib import cm\n",
    "import scipy\n",
    "import  itertools\n",
    "from datetime import datetime\n",
    "from seaborn import clustermap\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import time\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from dataLoader import *\n",
    "from basic_data_methods_helper import *\n",
    "from skbio.stats.distance import permanova, DistanceMatrix, anosim\n",
    "from skbio.stats.distance import mantel\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from matplotlib.collections import LineCollection\n",
    "from Bio import Phylo\n",
    "import re\n",
    "from statistics import mode\n",
    "import seaborn as sns\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis, CoxPHSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "from skbio.diversity.alpha import chao1\n",
    "\n",
    "# Set font for figures\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Arial']})\n",
    "\n",
    "# Base directory figure folders will be saved in \n",
    "path_to_save = '/Users/jendawk/Dropbox (MIT)/C Diff Recurrence Paper/Analyses/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/generic.py:5494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# set data_path to point to directory with data\n",
    "data_path = \"/Users/jendawk/Dropbox (MIT)/C Diff Recurrence Paper/Data/\"\n",
    "\n",
    "# Option to change filtering criteria\n",
    "dl = dataLoader(path = data_path, pt_perc = {'metabs': .25, '16s': .1, 'scfa': 0, 'toxin':0}, meas_thresh = \n",
    "                {'metabs': 0, '16s': 10, 'scfa': 0, 'toxin':0}, \n",
    "                var_perc = {'metabs': 50, '16s': 5, 'scfa': 0, 'toxin':0}, pt_tmpts = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metabs: 760\n",
      "16s: 237\n",
      "scfa: 8\n",
      "toxin: 2\n",
      "metabs_16s: 997\n",
      "metabs_16s_scfa: 997\n",
      "metabs_toxin: 762\n"
     ]
    }
   ],
   "source": [
    "# Number of features in each data set\n",
    "for key in dl.week.keys():\n",
    "    print(key + ': ' + str(dl.week[key][0]['x'].shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 1A - Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n",
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/core/computation/expressions.py:205: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n"
     ]
    }
   ],
   "source": [
    "targets = pd.Series(dl.targets_by_pt)\n",
    "targets = targets.replace('Cleared','Non-recurrer')\n",
    "targets = targets.replace('Recur', 'Recurrer')\n",
    "demo_dict = {}\n",
    "for feat in dl.demographics.columns.values:\n",
    "    if len(np.unique(dl.demographics[feat]))<=4 or (dl.demographics[feat].dtypes!=int and dl.demographics[feat].dtypes!=float):\n",
    "        for cat in np.unique(dl.demographics[feat]):\n",
    "            N = np.sum((dl.demographics[feat]==cat)*(targets=='Non-recurrer'))\n",
    "            c_re = np.sum((dl.demographics[feat]==cat)*(targets=='Recurrer'))\n",
    "            demo_dict[(feat,cat)] = {'N_nonRec': N, 'N_Rec': c_re}\n",
    "    else:\n",
    "        demo_dict[(feat,'Mean')] = {'N': np.mean(dl.demographics[feat])}\n",
    "        demo_dict[(feat,'STD')] = {'N': np.std(dl.demographics[feat])}\n",
    "        demo_dict[(feat,'Median')] = {'N': np.median(dl.demographics[feat])}\n",
    "        demo_dict[(feat,'Range')] = {'N': (np.min(dl.demographics[feat]),np.max(dl.demographics[feat]))}\n",
    "        \n",
    "df = pd.DataFrame(demo_dict).T\n",
    "if not os.path.isdir(path_to_save + 'output_tables'):\n",
    "    os.mkdir(path_to_save + 'output_tables')\n",
    "df.to_csv(path_to_save + 'output_tables/demographics_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 1B - # Recurrers at each timepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_dict = {}\n",
    "for week in dl.week['metabs'].keys():\n",
    "    rc_dict[week]={}\n",
    "    targets = dl.week['metabs'][week]['y']\n",
    "    rc_dict[week]['Recurrers'] = np.sum(targets=='Recurrer')\n",
    "    rc_dict[week]['Non-Recurrers'] = np.sum(targets=='Non-recurrer')\n",
    "if not os.path.isdir(path_to_save + 'output_tables'):\n",
    "    os.mkdir(path_to_save + 'output_tables')\n",
    "pd.DataFrame(rc_dict).T.to_csv(path_to_save + 'output_tables/outcome_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate analysis for demographics and clinical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jendawk/miniconda3/envs/CodeBase/lib/python3.7/site-packages/numpy/core/_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(path_to_save + 'univariate_analysis'):\n",
    "    os.mkdir(path_to_save + 'univariate_analysis')\n",
    "\n",
    "# Univariate analysis for demographics\n",
    "targets = dl.keys['metabs']['targets_by_pt']\n",
    "x = dl.demographics\n",
    "xx = dl.demographics[['Age','Sex','Race','BMI','Smoking status']]\n",
    "df_out = univariate_w_chi2(xx, targets)\n",
    "df_out.to_csv(path_to_save + 'univariate_analysis/demographics.csv')\n",
    "\n",
    "# Univariate analysis for clinical variables\n",
    "targets = dl.keys['metabs']['targets_by_pt']\n",
    "x = dl.clinical\n",
    "df_out = univariate_w_chi2(x, targets)\n",
    "df_out.to_csv(path_to_save + 'univariate_analysis/clinical.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export 16S data for DESeq2 in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(path_to_save + 'scripts/inputs/'):\n",
    "    os.mkdir(path_to_save + 'scripts/inputs/')\n",
    "    \n",
    "if not os.path.isdir(path_to_save + 'scripts/inputs/DEseq2'):\n",
    "    os.mkdir(path_to_save + 'scripts/inputs/DEseq2')\n",
    "\n",
    "weeks = [0,1,2]\n",
    "plot = True\n",
    "\n",
    "key = '16s'\n",
    "for week in weeks:\n",
    "#     if not isinstance(week, list):\n",
    "    x = dl.week_filt[key][week]['x']\n",
    "    y = (dl.week_filt[key][week]['y']=='Recurrer').astype('float')\n",
    "#     else:\n",
    "#         x,y,t = get_slope_data(dl.week_filt[key], week)\n",
    "        \n",
    "    if not os.path.isdir('inputs/DEseq2/'):\n",
    "        os.mkdir('inputs/DEseq2/')\n",
    "    x.to_csv(path_to_save +'scripts/inputs/DEseq2/counts_' + str(week).replace('.','-') + '.csv')\n",
    "    y.to_csv(path_to_save +'scripts/inputs/DEseq2/col_' + str(week).replace('.','-') + '.csv')\n",
    "    \n",
    "# Export for DEseq2 in R\n",
    "weeks = [(0,1),(1,2)]\n",
    "key = '16s'\n",
    "for outcome in ['Recurrer', 'Non-recurrer']:\n",
    "    for week_pair in weeks:\n",
    "        col0 = dl.week_filt[key][week_pair[0]]['x'].columns.values\n",
    "        col1 = dl.week_filt[key][week_pair[1]]['x'].columns.values\n",
    "        col_all = np.unique(np.concatenate([col0, col1]))\n",
    "        ix0, ix1 = dl.week_filt[key][week_pair[0]]['x'].index.values, dl.week_filt[key][week_pair[1]]['x'].index.values\n",
    "        \n",
    "        x0, y0 = dl.keys[key]['data'][col_all].loc[ix0], dl.week_filt[key][week_pair[0]]['y']\n",
    "        x1, y1 = dl.keys[key]['data'][col_all].loc[ix1], dl.week_filt[key][week_pair[1]]['y']\n",
    "        x0 = x0.loc[y0.values == outcome]\n",
    "        x1 = x1.loc[y1.values == outcome]\n",
    "        x = pd.concat([x0, x1])\n",
    "        y = [week_pair[0]]*x0.shape[0] + [week_pair[1]]*x1.shape[0]\n",
    "        y = pd.Series((np.array(y)==week_pair[1]).astype('float'), index = x.index.values)\n",
    "        if not os.path.isdir('inputs/DEseq2/'):\n",
    "            os.mkdir('inputs/DEseq2/')\n",
    "        x.to_csv(path_to_save +'scripts/inputs/DEseq2/counts_' + outcome +str(week_pair[0]) + '_' + str(week_pair[1]) + '.csv')\n",
    "        y.to_csv(path_to_save +'scripts/inputs/DEseq2/col_' + outcome + str(week_pair[0]) + '_' + str(week_pair[1]) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat DEseq2 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-293-6eced23a5336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdf_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_deseq2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdf_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'padj'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtaxa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_taxa_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_sorted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOTU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mdf_sorted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Taxa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox (MIT)/C Diff Recurrence Paper/Analyses/scripts/helper.py\u001b[0m in \u001b[0;36mreturn_taxa_names\u001b[0;34m(sequences, tax_dat)\u001b[0m\n\u001b[1;32m    462\u001b[0m                     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mtdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m                 \u001b[0mtd_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtdat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0mtd_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtd_out\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'nan'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2101\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m             \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2103\u001b[0;31m             \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_dedup_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CodeBase/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_maybe_dedup_names\u001b[0;34m(self, names)\u001b[0m\n\u001b[1;32m   1532\u001b[0m                     \u001b[0mcur_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1534\u001b[0;31m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1535\u001b[0m                 \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set path_deseq2 to path with R results\n",
    "path_deseq2 = path_to_save + 'univariate_analysis/16s/deseq2/'\n",
    "\n",
    "weeks = [0,1,2]\n",
    "plot = False\n",
    "\n",
    "key = '16s'\n",
    "    \n",
    "if not os.path.isdir(path_to_save + 'univariate_analysis/' + key):\n",
    "    os.mkdir(path_to_save + 'univariate_analysis/' + key)\n",
    "for file in os.listdir(path_deseq2):\n",
    "    if file == '.DS_Store':\n",
    "        continue\n",
    "    week = file.split('res')[-1].split('.')[0]\n",
    "    df_r = pd.read_csv(path_deseq2 + file, index_col = 0)\n",
    "    df_sorted = df_r.sort_values(by = 'padj')\n",
    "    taxa = return_taxa_names(df_sorted.OTU)\n",
    "    df_sorted.insert(1, \"Taxa\", taxa, allow_duplicates=True)\n",
    "    \n",
    "    df_sorted.insert(2,\"Direction\", np.zeros(df_sorted.shape[0]).astype(str), True)\n",
    "    \n",
    "    if '_' in file:\n",
    "        # Higher in cleared group (0) or first week\n",
    "        df_sorted[\"Direction\"].loc[df_sorted[\"log2fold\"]<0] = 'Higher in week ' + file.split('_')[0].split('rer')[1]\n",
    "\n",
    "        # Higher in recurred group (1) or second week\n",
    "        df_sorted[\"Direction\"].loc[df_sorted[\"log2fold\"]>0] = 'Higher in week ' + file.split('_')[1].split('.')[0]\n",
    "    else:\n",
    "        # Higher in cleared group (0) or first week\n",
    "        df_sorted[\"Direction\"].loc[df_sorted[\"log2fold\"]<0] = 'Decreased Risk'\n",
    "\n",
    "        # Higher in recurred group (1) or second week\n",
    "        df_sorted[\"Direction\"].loc[df_sorted[\"log2fold\"]>0] = 'Increased Risk'\n",
    "    \n",
    "    df_sorted = df_sorted.set_index(\"OTU\")\n",
    "    if plot:\n",
    "        for metab in df_sorted.index.values:\n",
    "            if df_sorted['padj'][metab] < .1:\n",
    "                plot_metab_over_time(metab, df_sorted['padj'][metab], dl.keys[key], path_to_save + 'univariate_analysis/' + \n",
    "                                     key + '/' + str(week) + '_deseq2_')\n",
    "    df_sorted.to_csv(path_to_save+'univariate_analysis/' + key + '/deseq2_' + key + str(week) + '.csv')\n",
    "    plt.figure()\n",
    "    plt.hist(df_sorted['padj'], bins = np.arange(0,1.05,.05))\n",
    "    plt.xlabel('p-values')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(key + ', week ' + str(week))\n",
    "    plt.savefig(path_to_save+'univariate_analysis/' + key + '/deseq2_' + key + str(week) + '.pdf')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate analysis for metabolites, SCFAs, and toxin/culture data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Univariate analysis for metabolites and SCFAs\n",
    "weeks = [0,1,2,[1,1.5,2]]\n",
    "week_pairs = [(0,1),(1,2)]\n",
    "plot = False\n",
    "\n",
    "for key in ['metabs', 'scfa','toxin']:\n",
    "    if not os.path.isdir(path_to_save + 'univariate_analysis/' + key):\n",
    "        os.mkdir(path_to_save + 'univariate_analysis/' + key)\n",
    "    if key != 'toxin':\n",
    "        continue\n",
    "    for outcome in ['Recurrer', 'Non-recurrer']:\n",
    "        for week_pair in week_pairs:\n",
    "            x0, y0 = dl.week[key][week_pair[0]]['x'], dl.week[key][week_pair[0]]['y']\n",
    "            x0 = x0.loc[y0.values == outcome]\n",
    "            x1, y1 = dl.week[key][week_pair[1]]['x'], dl.week[key][week_pair[1]]['y']\n",
    "            x1 = x1.loc[y1.values == outcome]\n",
    "\n",
    "            xboth = list(set(x0.columns.values).intersection(set(x1.columns.values)))\n",
    "            x = pd.concat([x0, x1])\n",
    "            if 'scfa' in key:\n",
    "                x = x.drop('Caproate', axis = 1)\n",
    "            y = [week_pair[0]]*x0.shape[0] + [week_pair[1]]*x1.shape[0]\n",
    "            y = pd.Series((np.array(y)==week_pair[1]).astype('float'), index = x.index.values)\n",
    "            df = univariate_w_chi2(x,y)\n",
    "            df2 = univariate_w_chi2(x,y, method = 'ttest')\n",
    "            if not os.path.isdir(path_to_save+'univariate_analysis/' + key ):\n",
    "                os.mkdir(path_to_save+'univariate_analysis/' + key)\n",
    "\n",
    "            df.insert(2,'Direction', np.zeros(df.shape[0]).astype(str))\n",
    "            df2.insert(2,'Direction', np.zeros(df2.shape[0]).astype(str))\n",
    "\n",
    "            # Higher in cleared group (0) or first week\n",
    "            df[\"Direction\"].loc[df[\"direction\"].astype(float)<0] = 'Higher in week ' + str(week_pair[0])\n",
    "            df2[\"Direction\"].loc[df2[\"direction\"].astype(float)<0] = 'Higher in week ' + str(week_pair[0])\n",
    "            # Higher in recurred group (1) or second week\n",
    "            df[\"Direction\"].loc[df[\"direction\"].astype(float)>0] = 'Higher in week ' + str(week_pair[1])\n",
    "            df2[\"Direction\"].loc[df2[\"direction\"].astype(float)>0] = 'Higher in week ' + str(week_pair[1])\n",
    "\n",
    "            df.to_csv(path_to_save+'univariate_analysis/' + key + '/ranksum_' + key + str(week_pair[0]) + '_' + \n",
    "                       str(week_pair[1]) + '_' + outcome + '.csv')\n",
    "            df2.to_csv(path_to_save+'univariate_analysis/' + key + '/ttest_' + key + str(week_pair[0]) + '_' + \n",
    "                       str(week_pair[1]) + '_' + outcome + '.csv')\n",
    "            plt.figure()\n",
    "            plt.hist(df2['BH corrected'], bins = np.arange(0,1.05,.05))\n",
    "            plt.xlabel('p-values')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title(key + ', week ' + str(week_pair))\n",
    "            plt.savefig(path_to_save+'univariate_analysis/' + key + '/ttest_' + key + str(week_pair[0]) + '_' + \n",
    "                       str(week_pair[1]) + '_' + outcome + '.pdf')\n",
    "            plt.close()\n",
    "#             with pd.ExcelWriter(path_save + 'Table 5 - Predictive Results.xlsx', mode = 'a') as writer:\n",
    "#                 df.to_excel(writer, sheet_name = 'A. Results')\n",
    "        \n",
    "    for week in weeks:\n",
    "        if not isinstance(week, list):\n",
    "            x, y = dl.week[key][week]['x'], dl.week[key][week]['y']\n",
    "            if 'scfa' in key:\n",
    "                x = x.drop('Caproate', axis = 1)\n",
    "            if 'toxin' in key:\n",
    "                x = x.iloc[:,-4:]\n",
    "            df = univariate_w_chi2(x,y)\n",
    "            df2 = univariate_w_chi2(x,y, method = 'ttest')\n",
    "        else:\n",
    "            x,y,t = get_slope_data(dl.week[key], week)\n",
    "            if 'scfa' in key:\n",
    "                x = x.drop('Caproate', axis = 1)\n",
    "            if 'toxin' in key:\n",
    "                x = x.iloc[:,-4:]\n",
    "            df = univariate_w_chi2(x,y)\n",
    "            df2 = univariate_w_chi2(x,y,method = 'ttest')\n",
    "            \n",
    "        if not os.path.isdir(path_to_save+'univariate_analysis/' + key):\n",
    "            os.mkdir(path_to_save+'univariate_analysis/' + key)\n",
    "        if plot:\n",
    "            for metab in df2.index.values:\n",
    "                if df2['BH corrected'][metab] < .05:\n",
    "                    try:\n",
    "                        plot_metab_over_time(metab, df2['BH corrected'][metab], dl.keys[key], path_to_save + 'univariate_analysis/' + \n",
    "                                             key + '/' + str(week) + '_')\n",
    "                        print('plotted ' + metab)\n",
    "                    except:\n",
    "                        plot_metab_over_time(metab, df2['BH corrected'][metab], dl.keys['metabs'], path_to_save + 'univariate_analysis/' + \n",
    "                                             key + '/' + str(week) + '_')\n",
    "                        print('plotted ' + metab)                    \n",
    "        \n",
    "        df.insert(2,'Direction', np.zeros(df.shape[0]).astype(str))\n",
    "        df2.insert(2,'Direction', np.zeros(df2.shape[0]).astype(str))\n",
    "        \n",
    "        # Higher in cleared group (0) or first week\n",
    "        df[\"Direction\"].loc[df[\"direction\"].astype(float)<0] = 'Decreased Risk'\n",
    "        df2[\"Direction\"].loc[df2[\"direction\"].astype(float)<0] = 'Decreased Risk'\n",
    "\n",
    "        # Higher in recurred group (1) or second week\n",
    "        df[\"Direction\"].loc[df[\"direction\"].astype(float)>0] = 'Increased Risk'\n",
    "        df2[\"Direction\"].loc[df2[\"direction\"].astype(float)>0] = 'Increased Risk'\n",
    "        \n",
    "        df.to_csv(path_to_save+'univariate_analysis/' + key + '/ranksum_' + key + str(week) + '.csv')\n",
    "        df2.to_csv(path_to_save+'univariate_analysis/' + key + '/ttest_' + key + str(week) + '.csv')\n",
    "        plt.figure()\n",
    "        plt.hist(df2['BH corrected'], bins = np.arange(0,1.05,.05))\n",
    "        plt.xlabel('p-values')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(key + ', week ' + str(week))\n",
    "        plt.savefig(path_to_save+'univariate_analysis/' + key + '/ttest_' + key + str(week) + '.pdf')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save compact univariate analysis to supplemental tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from string import ascii_uppercase\n",
    "path_univariate = path_to_save + 'univariate_analysis/'\n",
    "path_save = '/Users/jendawk/Dropbox (MIT)/C Diff Recurrence Paper/Supplemental Tables/'\n",
    "taxa_labels = pd.read_csv(path_to_save + 'scripts/inputs/' + 'taxa_labels.csv', index_col = [0])\n",
    "\n",
    "# dl.col_mat_mets = dl.col_mat_mets.set_index('BIOCHEMICAL')\n",
    "\n",
    "cs_groups = pd.read_csv(path_to_save + 'scripts/inputs/20200120_HumanCarbonSourceMap.csv', index_col = 0)\n",
    "cs_dict = {}\n",
    "for group in cs_groups.index.values:\n",
    "    cs_dict[group] = '; '.join(cs_groups.loc[group][1:].dropna())\n",
    "cs_dict_rev = {}\n",
    "for metabolite in dl.col_mat_mets.index.values:\n",
    "    loc = np.where(cs_groups==metabolite)[0]\n",
    "    if len(loc)>0:\n",
    "        cs_dict_rev[metabolite] = '; '.join(cs_groups.index.values[loc])\n",
    "    else:\n",
    "        cs_dict_rev[metabolite] = 'None'\n",
    "cs_map = pd.Series(cs_dict_rev)\n",
    "\n",
    "k = 3\n",
    "for dtype in os.listdir(path_univariate):\n",
    "    labs = []\n",
    "    df_out = {}\n",
    "#     if 'toxin' not in dtype:\n",
    "#         continue\n",
    "#     if dtype == '16s':\n",
    "#         k+= 1\n",
    "#         continue\n",
    "    fname = 'Univariate Analysis.xlsx'\n",
    "#     if os.path.exists(path_save + fname):\n",
    "#         os.remove(path_save + fname)\n",
    "    i = 0\n",
    "    if dtype == '.DS_Store':\n",
    "        continue\n",
    "    for file in os.listdir(path_univariate + dtype):\n",
    "        if 'kruskal' in file or 'ranksum' in file or '[' in file or '.DS_Store' in file:\n",
    "            continue\n",
    "        if '.csv' not in file:\n",
    "            continue\n",
    "#         print(i)\n",
    "#         print(file)\n",
    "        timepoint = file.split('.')[0].split(dtype)[1]\n",
    "        if 'rer' in timepoint:\n",
    "            if dtype == '16s':\n",
    "                label = timepoint.split('rer')[0] + 'rer;Week' + 'vs'.join(timepoint.split('rer')[1].split('_'))\n",
    "            else:\n",
    "                label = timepoint.split('_')[-1] + ';Week' + 'vs'.join(timepoint.split('_')[:-1])\n",
    "            lab_out = ('Intra-group',label)\n",
    "        else:\n",
    "            label = 'Week' + timepoint + ';R_vs_NR'\n",
    "            lab_out = ('Inter-group',label)\n",
    "        \n",
    "        df = pd.read_csv(path_univariate + dtype + '/' + file, index_col = [0])\n",
    "        labs.append(lab_out)\n",
    "        if dtype == '16s':\n",
    "            if np.sum(df['padj']<0.1)==0:\n",
    "                df = df.iloc[:10,:]\n",
    "            else:\n",
    "                df = df.loc[df['padj']<0.1]\n",
    "            seq_labels = taxa_labels['labels'].loc[df.index.values]\n",
    "            for ix in df.index.values:\n",
    "                df_out[(lab_out[0], lab_out[1], ix)] = {'Labels': seq_labels.loc[ix], 'FDR': df['padj'].loc[ix], \n",
    "                                                        'Direction': df['Direction'].loc[ix], \n",
    "                          'log2fold': df['log2fold'].loc[ix],\n",
    "                         'Taxonomy-RDP': taxa_labels['taxa_rdp'].loc[ix], \n",
    "                          }\n",
    "#             df_out[lab_out] = pd.DataFrame(df_out[lab_out])\n",
    "#             df_out[lab_out] = df_out[lab_out].set_index('Labels')\n",
    "        elif dtype == 'metabs':\n",
    "            if np.sum(df['BH corrected']<0.1)==0:\n",
    "                df = df.iloc[:10,:]\n",
    "            else:\n",
    "                df = df.loc[df['BH corrected']<0.1]\n",
    "            for ix in df.index.values:\n",
    "                df_out[(lab_out[0], lab_out[1], ix)] = {'FDR': df['BH corrected'].loc[ix], \n",
    "                                                        'Direction': df['Direction'].loc[ix], \n",
    "                          't-stat': df['t-stat'].loc[ix], 'Carbon Source Gp': cs_map.loc[df.index.values].loc[ix],\n",
    "                         'Super Pathway': dl.col_mat_mets['SUPER_PATHWAY'].loc[ix],\n",
    "                         'Sub Pathway': dl.col_mat_mets['SUB_PATHWAY'].loc[ix]}\n",
    "#             df_out[lab_out] = pd.DataFrame(df_out[lab_out])\n",
    "#             df_out[lab_out] = df_out[lab_out].set_index('names')\n",
    "        elif dtype == 'toxin':\n",
    "#             df = df.loc[df['BH corrected']<0.1]\n",
    "            for ix in df.index.values:\n",
    "                df_out[(lab_out[0], lab_out[1], ix)] = {'FDR': df['BH corrected'].loc[ix], \n",
    "                                   'Direction': df['Direction'].loc[ix], \n",
    "                               'test statistic': df['test statistic'].loc[ix],\n",
    "                     'method': df['method'].loc[ix]}\n",
    "            \n",
    "#             df_out[lab_out] = pd.DataFrame(df_out[lab_out])\n",
    "#             df_out[lab_out] = df_out[lab_out].set_index('data_type')\n",
    "        else:\n",
    "#             df = df.loc[df['BH corrected']<0.1]\n",
    "            for ix in df.index.values:\n",
    "                df_out[(lab_out[0], lab_out[1], ix)] = {'FDR': df['BH corrected'].loc[ix], \n",
    "                                   'Direction': df['Direction'].loc[ix], \n",
    "                               'test statistic': df['t-stat'].loc[ix]}\n",
    "            \n",
    "        \n",
    "    df_out = pd.DataFrame(df_out).T.sort_index()\n",
    "    if os.path.exists(path_save + fname):\n",
    "        with pd.ExcelWriter(path_save + fname, mode = 'a') as writer:\n",
    "            df_out.to_excel(writer, sheet_name = dtype.capitalize())\n",
    "    else:\n",
    "        with pd.ExcelWriter(path_save + fname) as writer:\n",
    "            df_out.to_excel(writer, sheet_name = dtype.capitalize())\n",
    "#         i+= 1\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save full univariate analysis to Data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from string import ascii_uppercase\n",
    "path_univariate = path_to_save + 'univariate_analysis/'\n",
    "path_save = '/Users/jendawk/Dropbox (MIT)/C Diff Recurrence Paper/Data/Univariate analyses/'\n",
    "taxa_labels = pd.read_csv(path_to_save + 'scripts/inputs/' + 'taxa_labels.csv', index_col = [0])\n",
    "\n",
    "# dl.col_mat_mets = dl.col_mat_mets.set_index('BIOCHEMICAL')\n",
    "\n",
    "cs_groups = pd.read_csv(path_to_save + 'scripts/inputs/20200120_HumanCarbonSourceMap.csv', index_col = 0)\n",
    "cs_dict = {}\n",
    "for group in cs_groups.index.values:\n",
    "    cs_dict[group] = '; '.join(cs_groups.loc[group][1:].dropna())\n",
    "cs_dict_rev = {}\n",
    "for metabolite in dl.col_mat_mets.index.values:\n",
    "    loc = np.where(cs_groups==metabolite)[0]\n",
    "    if len(loc)>0:\n",
    "        cs_dict_rev[metabolite] = '; '.join(cs_groups.index.values[loc])\n",
    "    else:\n",
    "        cs_dict_rev[metabolite] = 'None'\n",
    "cs_map = pd.Series(cs_dict_rev)\n",
    "\n",
    "k = 3\n",
    "for dtype in os.listdir(path_univariate):\n",
    "    fname = 'Table ' + str(k + 1) + '-' + dtype.capitalize() + ' Univariate Analysis.xlsx'\n",
    "\n",
    "    i = 0\n",
    "    if dtype == '.DS_Store':\n",
    "        continue\n",
    "    for file in os.listdir(path_univariate + dtype):\n",
    "        if 'kruskal' in file or 'ranksum' in file or '[' in file or '.DS_Store' in file:\n",
    "            continue\n",
    "        if '.csv' not in file:\n",
    "            continue\n",
    "#         print(i)\n",
    "#         print(file)\n",
    "        \n",
    "        df = pd.read_csv(path_univariate + dtype + '/' + file, index_col = [0])\n",
    "        if dtype == '16s':\n",
    "            seq_labels = taxa_labels['labels'].loc[df.index.values]\n",
    "            df_out = {'FDR': df['padj'], 'Direction': df['Direction'], \n",
    "                      'log2fold': df['log2fold'],'Taxonomy-Silva': taxa_labels['taxa_silva'].loc[df.index.values], \n",
    "                     'Taxonomy-RDP': taxa_labels['taxa_rdp'].loc[df.index.values], \n",
    "                      'Sequences': df.index.values, 'Labels': seq_labels}\n",
    "            df_out = pd.DataFrame(df_out)\n",
    "            df_out = df_out.set_index('Labels')\n",
    "        elif dtype == 'metabs':\n",
    "            df_out = {'FDR': df['BH corrected'], 'Direction': df['Direction'], \n",
    "                      't-stat': df['t-stat'], 'PubChem#': dl.col_mat_mets['PUBCHEM'].loc[df.index.values],\n",
    "                     'KEGG#': dl.col_mat_mets['PUBCHEM'].loc[df.index.values], 'HMDB#': \n",
    "                      dl.col_mat_mets['HMDB'].loc[df.index.values], 'Carbon Source Gp': cs_map.loc[df.index.values],\n",
    "                     'Super Pathway': dl.col_mat_mets['SUPER_PATHWAY'].loc[df.index.values],\n",
    "                     'Sub Pathway': dl.col_mat_mets['SUB_PATHWAY'].loc[df.index.values], 'names':df.index.values}\n",
    "            df_out = pd.DataFrame(df_out)\n",
    "            df_out = df_out.set_index('names')\n",
    "        elif dtype == 'toxin':\n",
    "            df_out = {'FDR': df['BH corrected'], 'Direction': df['Direction'], 'test statistic': df['test statistic'],\n",
    "                     'method': df['method']}\n",
    "            df_out = pd.DataFrame(df_out)\n",
    "        else:\n",
    "            df_out = {'FDR': df['BH corrected'], 'Direction': df['Direction'], 'test statistic': df['test statistic']}\n",
    "            df_out = pd.DataFrame(df_out)\n",
    "        \n",
    "        timepoint = file.split('.')[0].split(dtype)[1]\n",
    "\n",
    "        if 'rer' in timepoint:\n",
    "            if dtype == '16s':\n",
    "                label = timepoint.split('rer')[0] + 'rer;Week' + 'vs'.join(timepoint.split('rer')[1].split('_'))\n",
    "            else:\n",
    "                label = timepoint.split('_')[-1] + ';Week' + 'vs'.join(timepoint.split('_')[:-1])\n",
    "        else:\n",
    "            label = 'Week' + timepoint + ';R_vs_NR'\n",
    "        \n",
    "        nm = ascii_uppercase[i]\n",
    "        \n",
    "        if os.path.exists(path_save + fname):\n",
    "            with pd.ExcelWriter(path_save + fname, mode = 'a') as writer:\n",
    "                df_out.to_excel(writer, sheet_name = nm + '.' + label)\n",
    "        else:\n",
    "            with pd.ExcelWriter(path_save + fname) as writer:\n",
    "                df_out.to_excel(writer, sheet_name = nm + '.' + label)\n",
    "        i+= 1\n",
    "    k += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CodeBase)",
   "language": "python",
   "name": "codebase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
